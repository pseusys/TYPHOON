# TYPHOON

> Transfer Your Packets Hidden Over Observed Networks

There are lots of data transferring protocols out there.
Developers have made big progress in protecting user data with various cryptography algorithms, making breaking encryption a hard assignment.
Still, it's possible to try, and some progress is also being made e.g. on breaking asymmetrical ciphers.

This project tries to make another step in encryption: making a protocol so obfuscated that it's hard to identify and verify it in the first place.
Indeed, if an attacker doesn't know _what_ to decrypt the protocol, it makes breaking it significantly harder.

## Assumptions and limitations

There is one important assumption, required for proceeding with this protocol.
Making any message exchange undetectable requires continuous effort, meaning that literally all the patterns must be eliminated.
The patterns in question not only include any cleartext fields, but also encrypted fields with a well-known structure (i.e. that can be fingerprinted), messages being sent with a regular time interval, messages of the same length, etc.

That is why it was decided to base the protocol on UDP, since it operates raw network packets and does not require any packet size control.
Also, no certificate exchange handshake is proposed, because it would be impossible to encrypt completely, so a certificate should be pre-shared with all the users in advance via any third channels.

The protocol tries to mimic _generic protocol behavior_, that is common for most of the open-source protocols.
It specifically does not target any specific protocol, because it could be too hard to imitate it statistically precise.
If imitating a specific protocol is required, encrypted data should better embedded right into the protocol body (which is very possible for QUIC, DNS queries, etc.).

Finally, any flow control or reliable data delivery are not goals of the proposed protocol - they should be implemented by user applications using it.
Still, the protocol partly facilitates this goal by providing a health checking mechanism, so that a connection won't go down silently.

## Architecture

Every TYPHOON server or client consists of two separate parts: a session manager and a flow manager.
Session manager is responsible for maintaining the protocol status, encryption and health checks - but it doesn’t own any physical resources.
Flow manager is responsible for packet datagram flow obfuscation - one flow manager owns one UDP port.

In the simplest case (1 client - 1 server), flow manager is tightly coupled with session manager both on client and server side.
In a more complex scenario, a server can use several "proxy" flow managers, each of them having a separate port, IP address or even be deployed to a separate device.
In that case a client can select any number of these "proxies" to use - and create a separate flow manager to contact each of them.
While it is technically possible for these flow managers to operate using different IP addresses as well - using that in the real world remains questionable.

## Packet structure

There are two types of packets in TYPHOON protocol: real packets and decoy packets.
Real packets contain a real payload and a header, they are either processed or generated (in case of health checking packets) by the session manager.
The payload is encrypted with [marshalling encryption](#marshalling-encryption) using a session key.
The header is appended _after_ the payload, so here and after it will be called "tailor" instead.
It’s structure will be explained [below](#tailor-structure), and it is encrypted with [tailor encryption](#tailor-encryption) specified by the certificate, it is static **per server instance** (i.e. per server session manager).
The decoy packets, in turn, are just random bytes.

After a real packet body is constructed, it is passed to the flow manager.
Decoy packets are generated by the flow manager itself and are inserted right away.
Flow manager treats all the packets similarly: it prepends them with a [fake header](#fake-header) structure and a [fake body](#fake-body).
Both of these parameters are specified by certificate and are static **per server address** (i.e. per server flow manager).

Fake header and fake body structures are [defined in certificate](#certificate-structure), so that the client flow controller header would resemble the one on server side.

### Tailor structure

Tailor should always be positioned _at the very end_ of TYPHOON packet.
It is always `14` bytes long (plus constant encryption overhead defined by [tailor encryption](#tailor-encryption) algorithm used).
The tailor structure consists of the following fields:

| Field code | Field name | Byte length | Production meaning | Debug meaning |
| :---: | --- | :---: | --- | --- |
| **FG** | flags | `1` | Flags defining packet contents | - |
| **CD** | code | `1` | Client type in client handshake, handshake result in server handshake | - |
| **ID** | identity | `4` | Client version in client handshake, client identification number afterwards | - |
| **TM** | time | `4` | Delay before the next health check packet (milliseconds), unused for data packets | Packet sending timestamp |
| **PN** | packet number | `2` | Time-based packet number in health check packets, unused for data packets | Incremental packet number |
| **PL** | payload length | `2` | Length of encrypted packet payload | - |

See [debug mode description](#debug-mode) for more information on protocol debugging and how the header is interpreted differently in debug mode.
Packet flags can have the following values:

- `128`: handshake packet.
- `64`: health check packet.
- `32`: data packet.
- `16`: decoy packet.
- `8`: termination packet.

> NB! Normally only one of these values should be set, but there is an exception: a health check packet (that normally has an empty body) can be embedded into a data packet, that situation here and after is called "shadowride".

Tailor is the only part of the message that should be decrypted by the flow manager.
It always starts at `packet_length - 14 - tailor_encryption_overhead` and ends at the end of the packet.
If the data flag is set, the payload should be read starting from `packet_length - 14 - tailor_encryption_overhead - payload_length` and until the start of the tailor.
If the decoy flag is set, the packet should be discarded right away, by the flow manager.

### Fake body

Fake body is just a random-length string of random bytes.
It can be either empty, random or constant:

- `empty`: body is always empty, `0` bytes length.
- `random`: body length is random, it is bound between `TYPHOON_FAKE_BODY_LENGTH_MIN` and `TYPHOON_FAKE_BODY_LENGTH_MAX` constant values, making all the packets different in size.
- `constant`: body length depend on the other packet parts length and compliment them to a constant size.

> NB! Handling `constant` body length might not be trivial, as it imposes a strict limit on packet data contents length.
> TYPHOON protocol specifically does not support data fragmentation, so `constant` body length just won't have any effect if real packet body length is not always strictly limited.

### Fake header

Fake header is a special structure that mimics a header of some protocol sent in cleartext.
While the structure is generated by nature, it’s strictly described in the certificate for every server flow manager (unique IP address - port number tuple), so that all the traffic going to and from this address would have it uniform.

The header itself consists of a few fields, each field is either `1`, `2`, `4` or `8` bytes long (all primitive integer lengths), their total length in bytes is bound between `TYPHOON_FAKE_HEADER_LENGTH_MIN` and `TYPHOON_FAKE_HEADER_LENGTH_MAX` constants.
Each of these fields can be one of these types: none, random, constant, volatile, switching or incremental:

- `none`: header is always empty, `0` bytes length.
- `random`: fields have random values in every packet.
- `constant`: fields always have the same value (within a session).
- `volatile`: fields change their value with random time intervals.
- `switching`: fields change their value with set and predictable time intervals.
- `incremental`: fields add `1` to their value in every packet.

## Packet behavior

In general, the packet behavior is defined not only by packet type (explained below), but also by the source that the packet originates from.
The decoy packets are generated by the flow manager, and so they are sent directly as they are.
All the other packets are generated by the session manager - and so they should be delivered to one of the flow managers.

On the client side, the flow manager selection is weighted random.
Upon connection startup, a set of server proxies is selected and a flow manager with a random weight is created for every one of them.
During packet delivery, a random flow manager picks up the packet, where randomness is regulated by its weight.

On the server side, everything is a little more complex.
The server [keeps a set of clients](#client-management), but it is never notified about client proxy selection.
Whenever it receives a packet from a client, it updates a set of packet source addresses for the client, and whenever it wants to send a packet back, it selects the packet destination address from the set randomly (no weights are involved).

Finally, since server IP addresses and port numbers are static, clients can just send packets to it directly.
But it’s not the same for servers: according to UDP specification, the client IP address and port can change at any time.
That is why the server flow managers should maintain client number to client address mapping and update it upon receiving every packet (and decrypting its header).

### Data packets

The simplest pattern affects data packets: they are sent directly and completely, without any delays, jitter, splitting or combination - as soon as possible, providing maximum efficiency.

Just like it has already been described in [packet structure](#packet-structure) section, every data packet is encrypted, prefixed with a [fake header](#fake-header) and postfixed with an [encrypted tailor](#tailor-structure).

### Handshake packets

The TYPHOON protocol relies on a two-way handshake that closely resembles one of OBFS4 and NTORv3 protocols.
There is no reason to wait for the third packet form client (TCP-style) as it is not possible to overload server with partially-initialized sessions, because [only one active connection is allowed per certificate](#initial-data-handling).
Moreover, that means that if the client sends a handshake packet on an existing connection, its internal state gets silently reset (see [health check packet description](#health-check-packets) for more information on connection internal state).
See [handshake encryption specification](#handshake-encryption) for cryptographic details of the handshake.

An important requirement for the handshake is that it is indistinguishable from any other subsequent packet flow.
The packets destinations, lengths and contents should be similar to data and decoy packets.
That's why, technically it is not even necessary that the first packet of the session is handshake (instead, some decoy packets can go first).
Still, data packets can only go after the client receives the second handshake message.

The handshake packets carry implementation-dependant encrypted initial data and also are prefixed with a [fake header](#fake-header) and postfixed with an [encrypted tailor](#tailor-structure).
In terms of behavior, the packets are treated just like the [health check packets](#health-check-packets), meaning that the server handshake response does not arrive immediately, but instead with a [next in delay](#next-in-computation) (this delay however is multiplied by `TYPHOON_HANDSHAKE_NEXT_IN_FACTOR` constant).
Also, just like the health check packets, handshake packets are retransmitted.

### Health check packets

Health check packets are used for stale connections tracking.
In case client and server fail to perform health check exchange with each other for a few times in a row, the connection terminates with an error on both sides.
That is why the health check packets, just like [it was mentioned earlier](#assumptions-and-limitations) and unlike the [data packets](#data-packets), are transmitted _reliably_.

In order to implement this reliability, every connection maintains an internal state both on client and server, that is synchronized by the health check packets.
The internal state is designed to be as simple as possible in order to preserve high protocol efficiency and avoid complex state preserving logic.
The state itself is dictated by client, and the server simply follows it.

#### Decay cycle

Health checking cycle is also here and after called "decay" cycle.
The decay behavior is mostly defined by [**TM** and **PN** fields of the tailor](#tailor-structure).

Client initiates the exchange (the first health check exchange is embedded into handshake), setting **PN** field to the current unix timestamp (in seconds) and **TM** to a [random next in delay](#next-in-computation).
The **PN** field value is remembered and used as the _current health check packet number_.
After that client sleeps for server response for **TM** seconds plus [timeout value](#timeout-computation).
If it receives a health check message from server with unexpected packet number (either during waiting or sleeping), it will be silently discarded.

Server receives the health check packet and updates its remembered _current health check packet number_.
It waits for **TM** seconds (maybe [not exactly](#packet-shadowriding)) before responding - and then constructs a response health check message with remembered health check packet number as **PN** and **TM** set to a [random next in delay](#next-in-computation).
After that server sleeps for client response for **TM** seconds plus [timeout value](#timeout-computation).
Whenever it receives a new health check message from client (either during waiting or sleeping), it restarts all over unconditionally.

Client receives the server response, waits for **TM** seconds (maybe [not exactly](#packet-shadowriding)) and starts the health checking over.

If client receives a valid response while sleeping or if a server receives any response while sleeping, they wake up (at some point) and restart their part of decay.
If they do not, they increase an internal counter until `TYPHOON_MAX_RETRIES` is reached, which means that one of the parties has most likely disappeared.
In that case the connection eventually decays completely with an error.

#### Packet shadowriding

When a client or a server waits before sending a next health check packet, it does not exactly wait for all the next in delay and then just send the packet directly.
Instead, it attempts to attach the health check header to any data packet passing through.

In order to do that, it sleeps for `next_in - smooth_RTT` and then waits for `smooth_RTT * 2` for any data packet (see [how RTT is computed](#rtt-computation)).
In case a packet arrives, the health check message will be directly written into the header.
Otherwise, a separate empty packet with the health checking header will be sent.

This allows avoiding sending any extra packets, which can be specifically useful if many small data packets are being sent continuously.

> NB! This extra delay is already included into timeout value, so there is no need to wait for one more RTT on the other end.
> The receiver side should wait just for `next_in + timeout`.

#### Next in computation

Next in is a random delay (in milliseconds) before the next health check response is expected to be sent.
It is bound between `TYPHOON_HEALTH_CHECK_NEXT_IN_MIN` and `TYPHOON_HEALTH_CHECK_NEXT_IN_MAX` constants.

> NB! This value is one of the ways client explicitly **dictates** server behavior, so it should be handled with extra care.
> In particular, next in value should be clamped between the constant boundaries not only on client, _but also_ on server.
> Otherwise, a careless client could set server decay iteration delay to almost 50 days (maximum value of **TM** field in milliseconds).

Normally, next in should be always greater than timeout.
It also should be much grater (something like 5 times) than RTT.
By default it is enforced by constants, and this dependency should be kept if the constants are changed.

#### Timeout computation

Timeout is normally calculated as a derivative from the RTT of a handshake packet between client and server.
However, in case no RTT is available yet (i.e. for the handshake and first health check packets) it takes `TYPHOON_TIMEOUT_DEFAULT` value.
Whenever RTT is initialized, the timeout will be calculated as `(smooth_RTT + RTT_variance) * TYPHOON_TIMEOUT_RTT_FACTOR` (the latter is a constant).
In every case, timeout value should be clamped between `TYPHOON_TIMEOUT_MIN` and `TYPHOON_TIMEOUT_MAX`.

#### RTT computation

RTT is calculated using [EWMA](https://en.wikipedia.org/wiki/Moving_average#Exponential_moving_average) algorithm ([smooth RTT](https://patents.google.com/patent/US20100061236A1/en)), constants `TYPHOON_RTT_ALPHA` and `TYPHOON_RTT_BETA` are used for its calculation.
Only health check packets are used for RTT calculation, since data packets do not have any defined packet number and handshake packet processing can take too much time and distort RTT value.

> NB! It is important to subtract the next in delay from the single packet RTT on every step.
> It should be calculated somehow like this: `new_packet_receive_time - last_packet_send_time - last_packet_next_in`.

If RTT is not initialized yet, the `TYPHOON_RTT_DEFAULT` should be used instead.
In every case, RTT value should be clamped between `TYPHOON_RTT_MIN` and `TYPHOON_RTT_MAX`.

### Decoy packets

Decoy packets are the most versatile, but also volatile of all the packet types.
They are invisible and not controlled by the session manager, instead they are fully processed by flow manager: created, sent and discarded.

In general, flow manager outputs two types of decoy packets: ordinary and "maintenance".
While ordinary packets mimic just any encrypted data packets, maintenance packets try to replicate behavior of health check packets of other protocols.
There are some exceptions, in general the decoy packets are more likely to appear when there are only a few data packets being sent - and less likely to appear in times of data packet bursts.

Decoy packet exchange behavior is configured for every flow manager and should match for client and server flow manager - so it is also included into [the certificate](#certificate-structure).
The configuration includes:

- Communication mode: defines general decoy packet sending behavior, possible values are: `heavy`, `noisy`, `sparse`, `smooth`.
- Maintenance mode: defines the way how maintenance packets would look like, possible values are: `none`, `timed`, `sized`, `both`.
- Replication mode: defines what packets will be duplicated, possible values are: `none`, `maintenance`, `all`.
- Subheader pattern: defines whether the maintenance packets should have their own fake header, boolean.

#### Communication mode

Communication mode defines a few important parameters used for decoy packet generation.

TODO!

#### Maintenance mode

TODO!

#### Replication mode

TODO!

#### Subheader pattern

TODO!

## Cryptography

TODO!

Still, there are some differences, coming from different requirements to the handshake:

1. For the sake of future-proof safety it was decided to implement the handshake using a post-quantum algorithm instead of pure elliptic-curve cryptography.
2. The handshake messages should also be obfuscated with no clear structure that could be fingerprinted.
3. Since no runtime certificate exchange is defined, the handshake procedure can rely on almost unlimited-length pre-defined data.

Unfortunately, right now all these requirements can not be achieved at once, because there is no known way to hide the internal structure of any of the post-quantum ciphers.

### Handshake encryption

TODO!

### Tailor encryption

TODO!

### Marshalling encryption

TODO!

### Certificate structure

TODO!

## Proposed implementation

TODO!

### Sockets and listeners

TODO!

### Client management

TODO!

### Initial data handling

TODO!

### Handshake error codes

TODO!

### Constants and defaults

The constant default values were selected from experience.
Still, there are some important universal values to consider:

- Normally, the `MTU` value is around 1500 bytes, so it might be useful to keep datagram length below that value to avoid IP-level fragmentation.
- Many applications keep timeouts at around half of a minute, so having timeout around that value might improve user experience.
- Packet retransmission attempts number varies around 10, so a value like that should be selected for the sake of safety.

These constants are used in some of the protocol values computation:

| Constant | Meaning | Default |
| --- | --- | :---: |
| `TYPHOON_FAKE_BODY_LENGTH_MIN` | Minimum length of the fake body random byte string | `0` |
| `TYPHOON_FAKE_BODY_LENGTH_MAX` | Maximum length of the fake body random byte string | `1024` |
| `TYPHOON_FAKE_HEADER_LENGTH_MIN` | Minimum length of the fake header structure | `4` |
| `TYPHOON_FAKE_HEADER_LENGTH_MAX` | Maximum length of the fake header structure | `32` |
| `TYPHOON_HEALTH_CHECK_NEXT_IN_MIN` | Minimum delay between health checking packets | `64000` |
| `TYPHOON_HEALTH_CHECK_NEXT_IN_MAX` | Maximum delay between health checking packets | `256000` |
| `TYPHOON_HANDSHAKE_NEXT_IN_FACTOR` | During handshake, next in values will be multiplied by this number | `0.02`, making minimal next in equal to `1.3` seconds and maximal next in equal to `5.1` seconds |
| `TYPHOON_MAX_RETRIES` | Maximum number of decay cycle iterations before protocol failure is registered | `12` |
| `TYPHOON_TIMEOUT_DEFAULT` | Default decay timeout value, used in cases when RTT is not available | `30000` |
| `TYPHOON_TIMEOUT_MIN` | Minimum decay timeout value | `4000` |
| `TYPHOON_TIMEOUT_MAX` | Maximum decay timeout value | `32000` |
| `TYPHOON_TIMEOUT_RTT_FACTOR` | If RTT is available, it will be multiplied by this value in order to receive timeout | `5` |
| `TYPHOON_RTT_ALPHA` | Alpha constant for SRTT algorithm | `0.125` |
| `TYPHOON_RTT_BETA` | Beta constant for SRTT algorithm | `0.25` |
| `TYPHOON_RTT_DEFAULT` | Default RTT value, used in cases when no packet roundtrip was registered yet | `5` |
| `TYPHOON_RTT_MIN` | Minimum RTT value | `1000` |
| `TYPHOON_RTT_MAX` | Maximum RTT value | `8000` |

> Applying `TYPHOON_HANDSHAKE_NEXT_IN_FACTOR` to handshake next in makes minimal next in equal to `1.3` seconds and maximal next in equal to `5.1` seconds.
> These values are not clamped by any constant bounds.

A protocol implementation should allow overriding these values at runtime.
Still keep in mind that it might be dangerous because some of these constants affect both client and server behavior at the same time.
As a final attempt, an implementation should attempt to read these constants from environment during initialization.

There are also some deduced constants, defined by some external factors rather than selected values:

| Constant | Value |
| --- | :---: |
| Tailor length | `14` |

TODO!

### Debug mode

TODO!

## Code and tests

TODO!

## Development

TODO!
